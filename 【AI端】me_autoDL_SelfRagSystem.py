#!/usr/bin/env python
# coding: utf-8

# In[2]:


# æŸ¥çœ‹å½“å‰æŒ‚è½½çš„æ•°æ®é›†ç›®å½•, è¯¥ç›®å½•ä¸‹çš„å˜æ›´é‡å¯ç¯å¢ƒåä¼šè‡ªåŠ¨è¿˜åŸ
# View dataset directory. 
# This directory will be recovered automatically after resetting environment. 
#get_ipython().system('ls /home/aistudio/data')


# In[58]:


#wuxg@2025.12.14ï¼š  local å‘é‡åŒ–æ–¹å¼ã€å·²å¼ƒç”¨ã€‘
# !pip uninstall -y paddlenlp paddlepaddle


# In[28]:


#wuxg@2025.12.14ï¼š  local å‘é‡åŒ–æ–¹å¼ã€å·²å¼ƒç”¨ã€‘
#!pip install paddlepaddle==2.5.2 -i https://mirror.baidu.com/pypi/simple
#!pip install paddlenlp==2.6.0 -i https://mirror.baidu.com/pypi/simple


# In[26]:


#wuxg@2025.12.14ï¼š  local å‘é‡åŒ–æ–¹å¼ã€å·²å¼ƒç”¨ã€‘
## 1. æŸ¥çœ‹Pythonç‰ˆæœ¬å’Œä½æ•°ï¼Œç¡®è®¤æ˜¯64ä½
# !python -c "import sys; print('Pythonç‰ˆæœ¬:', sys.version); print('æ˜¯å¦64ä½:', sys.maxsize > 2**32)"

## 2. æŸ¥çœ‹pipæºä¸Šå¯ç”¨çš„paddlepaddleç‰ˆæœ¬
# !pip index versions paddlepaddle

## 3. æŸ¥çœ‹pipæºä¸Šå¯ç”¨çš„paddlenlpç‰ˆæœ¬
# !pip index versions paddlenlp


# In[24]:


#wuxg@2025.12.14ï¼š  local å‘é‡åŒ–æ–¹å¼ã€å·²å¼ƒç”¨ã€‘
## å®‰è£…paddlepaddleï¼Œä¿¡ä»»mirror.baidu.com
#!pip install paddlepaddle -i https://mirror.baidu.com/pypi/simple --trusted-host mirror.baidu.com

## å®‰è£…paddlenlpï¼Œä¿¡ä»»mirror.baidu.com
#!pip install paddlenlp -i https://mirror.baidu.com/pypi/simple --trusted-host mirror.baidu.com


# In[ ]:


#wuxg@2025.12.14ï¼š  local å‘é‡åŒ–æ–¹å¼ã€å·²å¼ƒç”¨ã€‘
#import paddle
#from paddlenlp.transformers import ErnieTokenizer, ErnieModel
#print(f"PaddlePaddle ç‰ˆæœ¬: {paddle.__version__}")


# In[23]:


# æŸ¥çœ‹å·¥ä½œåŒºæ–‡ä»¶ï¼Œè¯¥ç›®å½•ä¸‹é™¤dataç›®å½•å¤–çš„å˜æ›´å°†ä¼šæŒä¹…ä¿å­˜ã€‚è¯·åŠæ—¶æ¸…ç†ä¸å¿…è¦çš„æ–‡ä»¶ï¼Œé¿å…åŠ è½½è¿‡æ…¢ã€‚
# View personal work directory. 
# All changes, except /data, under this directory will be kept even after reset. 
# Please clean unnecessary files in time to speed up environment loading. 
#get_ipython().system('ls /home/aistudio')


# In[20]:


# å¦‚æœéœ€è¦è¿›è¡ŒæŒä¹…åŒ–å®‰è£…, éœ€è¦ä½¿ç”¨æŒä¹…åŒ–è·¯å¾„, å¦‚ä¸‹æ–¹ä»£ç ç¤ºä¾‹:
# If a persistence installation is required, 
# you need to use the persistence path as the following: 
#get_ipython().system('mkdir /home/aistudio/external-libraries')
#get_ipython().system('pip install beautifulsoup4')


# In[22]:


# åŒæ—¶æ·»åŠ å¦‚ä¸‹ä»£ç , è¿™æ ·æ¯æ¬¡ç¯å¢ƒ(kernel)å¯åŠ¨çš„æ—¶å€™åªè¦è¿è¡Œä¸‹æ–¹ä»£ç å³å¯: 
# Also add the following code, 
# so that every time the environment (kernel) starts, 
# just run the following code: 
import sys 
sys.path.append('/home/aistudio/external-libraries')


# In[14]:


#get_ipython().system('pip install erniebot==0.5.3')
#get_ipython().system('pip install langchain==0.1.11')
#get_ipython().system('pip install langgraph==0.0.26')


# In[18]:


#wuxg@2025.12.14ï¼š  local å‘é‡åŒ–æ–¹å¼ã€å·²å¼ƒç”¨ã€‘
#!pip install openai pandas numpy faiss-cpu sentence-transformers paddle


# In[8]:


#get_ipython().system('pip install faiss-gpu')


# In[33]:


#pip list | grep faiss


# In[16]:


#wuxg@2025.12.14ï¼š  local å‘é‡åŒ–æ–¹å¼ã€å·²å¼ƒç”¨ã€‘
## è®¾å¤‡é…ç½®
#print("âœ… GPUå¯ç”¨çŠ¶æ€ï¼š", paddle.is_compiled_with_cuda())
#device = "gpu" if paddle.is_compiled_with_cuda() else "cpu"
#paddle.set_device(device)


# In[32]:


#pip install pypdf


# In[56]:


import warnings
warnings.filterwarnings("ignore")
import numpy as np
import faiss
from pypdf import PdfReader
import pandas as pd


#from docx import Document
from typing import List, Dict, Tuple, Optional, Any, Union, Callable, TypedDict  # å¯¼å…¥å¸¸ç”¨çš„ç±»å‹æç¤º


# In[36]:
import os


os.environ["WUXG_API_KEY"] = "678824fbafa46a532fdc555d378ab76d81c768aa"
api_key=os.environ.get("WUXG_API_KEY")


# In[34]:





# In[38]:



# ============================================
# 1. æ–‡æœ¬å·¥å…·ç±» (TextProcessor)
# ============================================

class TextProcessor:
    """
    æ–‡æœ¬å¤„ç†å·¥å…·ç±»ï¼Œç”¨äºä»å„ç§æ–‡ä»¶æ ¼å¼ä¸­æå–å’Œå¤„ç†æ–‡æœ¬ã€‚ç»Ÿä¸€çš„æ–‡æœ¬æå–å’Œå¤„ç†æ¥å£ã€‚
	- ç‰¹ç‚¹ ï¼š
	  - æ”¯æŒ PDFã€Excelã€Wordã€TXT å¤šç§æ ¼å¼
	  - æ™ºèƒ½æ–‡æœ¬åˆ†å—ï¼ˆæ”¯æŒæŒ‰æ®µè½å’Œå›ºå®šå¤§å°ï¼‰
	  - æ‰¹é‡æ–‡ä»¶å¤„ç†èƒ½åŠ›
	  - é”™è¯¯å¼‚å¸¸å¤„ç†
	  - å¯é…ç½®çš„å—å¤§å°
    """
    
    def __init__(self, chunk_size: int = 500):
        """
        åˆå§‹åŒ–æ–‡æœ¬å¤„ç†å™¨
        
        å‚æ•°:
        - chunk_size: æ–‡æœ¬å—å¤§å°ï¼Œé»˜è®¤500å­—ç¬¦
        """
        self.chunk_size = chunk_size
        self.supported_formats = ['.pdf', '.xlsx', '.xls', '.docx', '.doc', '.txt']
        
    def extract_from_pdf(self, pdf_path: str) -> List[str]:
        """
        ä»PDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬å¹¶åˆ†å—
        
        å‚æ•°:
        - pdf_path: PDFæ–‡ä»¶è·¯å¾„
        
        è¿”å›:
        - æ–‡æœ¬å—åˆ—è¡¨
        """
        if not os.path.exists(pdf_path):
            raise FileNotFoundError(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
            
        try:
            reader = PdfReader(pdf_path)
            text = ""
            for page in reader.pages:
                extracted = page.extract_text()
                if extracted:
                    text += extracted + "\n"
            
            if not text.strip():
                print(f"âš ï¸ PDFæ–‡ä»¶ {pdf_path} æœªæå–åˆ°æ–‡æœ¬")
                return []
                
            # åˆ†å—å¤„ç†
            chunks = self._chunk_text(text)
            print(f"âœ… ä»PDFæå– {len(chunks)} ä¸ªæ–‡æœ¬å—: {pdf_path}")
            return chunks
            
        except Exception as e:
            print(f"âŒ PDFæå–å¤±è´¥ {pdf_path}: {e}")
            return []
    
    def extract_from_excel(self, excel_path: str) -> List[str]:
        """
        ä»Excelæ–‡ä»¶ä¸­æå–æ–‡æœ¬å¹¶åˆ†å—
        
        å‚æ•°:
        - excel_path: Excelæ–‡ä»¶è·¯å¾„
        
        è¿”å›:
        - æ–‡æœ¬å—åˆ—è¡¨
        """
        if not os.path.exists(excel_path):
            raise FileNotFoundError(f"Excelæ–‡ä»¶ä¸å­˜åœ¨: {excel_path}")
            
        try:
            excel_file = pd.ExcelFile(excel_path)
            all_chunks = []
            
            for sheet in excel_file.sheet_names:
                try:
                    df = pd.read_excel(excel_file, sheet)
                    sheet_text = df.to_string()
                    
                    if sheet_text.strip():
                        sheet_chunks = self._chunk_text(sheet_text)
                        all_chunks.extend(sheet_chunks)
                        print(f"  ğŸ“Š å·¥ä½œè¡¨ '{sheet}': {len(sheet_chunks)} ä¸ªå—")
                    else:
                        print(f"  âš ï¸ å·¥ä½œè¡¨ '{sheet}' ä¸ºç©º")
                        
                except Exception as e:
                    print(f"  âš ï¸ è¯»å–å·¥ä½œè¡¨ '{sheet}' å¤±è´¥: {e}")
                    continue
            
            print(f"âœ… ä»Excelæå– {len(all_chunks)} ä¸ªæ–‡æœ¬å—: {excel_path}")
            return all_chunks
            
        except Exception as e:
            print(f"âŒ Excelæå–å¤±è´¥ {excel_path}: {e}")
            return []
    
    def extract_from_word(self, word_path: str) -> List[str]:
        """
        ä»Wordæ–‡ä»¶ä¸­æå–æ–‡æœ¬å¹¶åˆ†å—
        
        å‚æ•°:
        - word_path: Wordæ–‡ä»¶è·¯å¾„
        
        è¿”å›:
        - æ–‡æœ¬å—åˆ—è¡¨
        """
        if not os.path.exists(word_path):
            raise FileNotFoundError(f"Wordæ–‡ä»¶ä¸å­˜åœ¨: {word_path}")
            
        try:
            doc = Document(word_path)
            text = ""
            for para in doc.paragraphs:
                if para.text.strip():
                    text += para.text + "\n"
            
            if not text.strip():
                print(f"âš ï¸ Wordæ–‡ä»¶ {word_path} æœªæå–åˆ°æ–‡æœ¬")
                return []
                
            # åˆ†å—å¤„ç†
            chunks = self._chunk_text(text)
            print(f"âœ… ä»Wordæå– {len(chunks)} ä¸ªæ–‡æœ¬å—: {word_path}")
            return chunks
            
        except Exception as e:
            print(f"âŒ Wordæå–å¤±è´¥ {word_path}: {e}")
            return []
    
    def extract_from_text(self, text: str) -> List[str]:
        """
        ä»çº¯æ–‡æœ¬ä¸­æå–å¹¶åˆ†å—
        
        å‚æ•°:
        - text: åŸå§‹æ–‡æœ¬
        
        è¿”å›:
        - æ–‡æœ¬å—åˆ—è¡¨
        """
        if not text.strip():
            return []
            
        chunks = self._chunk_text(text)
        print(f"âœ… ä»æ–‡æœ¬æå– {len(chunks)} ä¸ªæ–‡æœ¬å—")
        return chunks
    
    def extract_from_file(self, file_path: str) -> List[str]:
        """
        æ ¹æ®æ–‡ä»¶æ‰©å±•åè‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æå–æ–¹æ³•
        
        å‚æ•°:
        - file_path: æ–‡ä»¶è·¯å¾„
        
        è¿”å›:
        - æ–‡æœ¬å—åˆ—è¡¨
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        file_ext = os.path.splitext(file_path)[1].lower()
        
        if file_ext == '.pdf':
            return self.extract_from_pdf(file_path)
        elif file_ext in ['.xlsx', '.xls']:
            return self.extract_from_excel(file_path)
        elif file_ext in ['.docx', '.doc']:
            return self.extract_from_word(file_path)
        elif file_ext == '.txt':
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            return self.extract_from_text(content)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}ï¼Œæ”¯æŒæ ¼å¼: {self.supported_formats}")
    
    def extract_from_multiple_files(self, file_paths: List[str]) -> Dict[str, List[str]]:
        """
        ä»å¤šä¸ªæ–‡ä»¶ä¸­æ‰¹é‡æå–æ–‡æœ¬
        
        å‚æ•°:
        - file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        
        è¿”å›:
        - å­—å…¸ï¼Œé”®ä¸ºæ–‡ä»¶è·¯å¾„ï¼Œå€¼ä¸ºæ–‡æœ¬å—åˆ—è¡¨
        """
        results = {}
        for file_path in file_paths:
            try:
                chunks = self.extract_from_file(file_path)
                results[file_path] = chunks
            except Exception as e:
                print(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥ {file_path}: {e}")
                results[file_path] = []
        
        return results
    
    def _chunk_text(self, text: str) -> List[str]:
        """
        å†…éƒ¨æ–¹æ³•ï¼šå°†æ–‡æœ¬åˆ†å—
        
        å‚æ•°:
        - text: åŸå§‹æ–‡æœ¬
        
        è¿”å›:
        - æ–‡æœ¬å—åˆ—è¡¨
        """
        if not text.strip():
            return []
        
        # å…ˆæŒ‰æ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
        
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            # å¦‚æœæ®µè½æœ¬èº«å°±å¾ˆå¤§ï¼Œç›´æ¥åˆ†å‰²
            if len(para) >= self.chunk_size:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                
                # åˆ†å‰²å¤§æ®µè½
                for i in range(0, len(para), self.chunk_size):
                    chunk = para[i:i + self.chunk_size]
                    if chunk.strip():
                        chunks.append(chunk.strip())
            else:
                # å¦‚æœå½“å‰å—åŠ ä¸Šæ–°æ®µè½ä¸è¶…è¿‡å¤§å°ï¼Œå°±åˆå¹¶
                if len(current_chunk) + len(para) + 1 <= self.chunk_size:
                    if current_chunk:
                        current_chunk += "\n" + para
                    else:
                        current_chunk = para
                else:
                    # å¦åˆ™ä¿å­˜å½“å‰å—ï¼Œå¼€å§‹æ–°å—
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = para
        
        # å¤„ç†æœ€åä¸€ä¸ªå—
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks


# In[40]:


#wuxg@2025.12.14: å·²å¼ƒç”¨ localå‘é‡åŒ–æ–¹å¼
#from paddlenlp.transformers import ErnieTokenizer, ErnieModel


# In[50]:



# ============================================
# 2. å‘é‡åŒ–ç±» (Vectorizer)
# ============================================
#wuxg@2025.12.14 ï¼š æœ¬å‘é‡åŒ–æ–¹å¼ ã€å¤±è´¥ã€‚å°†æ”¹ä¸ºaistudioçš„åœ¨çº¿å‘é‡åŒ–æ–¹å¼ï¼ä½¿ç”¨ä¸‹é¢çš„ERNIEVectorizer2ã€‘ã€å·²å¼ƒç”¨ã€‘
class ERNIEVectorizer1:
    """
    ERNIEæ¨¡å‹å‘é‡åŒ–ç±»ï¼Œç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºã€‚ä½¿ç”¨ERNIEæ¨¡å‹è¿›è¡Œæ–‡æœ¬å‘é‡åŒ–
	-  ç‰¹ç‚¹ ï¼š
	  - æ”¯æŒæ‰¹é‡å‘é‡åŒ–å¤„ç†
	  - è¿›åº¦æ˜¾ç¤ºå’Œé”™è¯¯å¤„ç†
	  - å¯é…ç½®çš„æ‰¹å¤„ç†å¤§å°å’Œæ–‡æœ¬é•¿åº¦
	  - æä¾›æ¨¡å‹ä¿¡æ¯æŸ¥è¯¢
	  - æ”¯æŒå•ä¸ªæ–‡æœ¬å‘é‡åŒ–
    """
    
    def __init__(self, model_name: str = "ernie-3.0-medium-zh", 
                 batch_size: int = 16, max_length: int = 512):
        """
        åˆå§‹åŒ–ERNIEå‘é‡åŒ–å™¨
        
        å‚æ•°:
        - model_name: ERNIEæ¨¡å‹åç§°
        - batch_size: æ‰¹å¤„ç†å¤§å°
        - max_length: æœ€å¤§æ–‡æœ¬é•¿åº¦
        """
        print(f"ğŸ”§ åˆå§‹åŒ–ERNIEå‘é‡åŒ–å™¨: {model_name}")
        self.model_name = model_name
        self.batch_size = batch_size
        self.max_length = max_length
        self.embedding_dim = 768  # ERNIE-3.0-medium-zhçš„å‘é‡ç»´åº¦
        
        # åŠ è½½tokenizerå’Œæ¨¡å‹
        try:
            self.tokenizer = ErnieTokenizer.from_pretrained(model_name)
            self.model = ErnieModel.from_pretrained(model_name)
            self.model.eval()
            print(f"âœ… ERNIEæ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‘é‡ç»´åº¦: {self.embedding_dim}")
        except Exception as e:
            print(f"âŒ ERNIEæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def vectorize(self, text_chunks: List[str]) -> np.ndarray:
        """
        å°†æ–‡æœ¬å—åˆ—è¡¨å‘é‡åŒ–
        
        å‚æ•°:
        - text_chunks: æ–‡æœ¬å—åˆ—è¡¨
        
        è¿”å›:
        - å‘é‡çŸ©é˜µï¼Œå½¢çŠ¶ä¸º (n_samples, embedding_dim)
        """
        if not text_chunks:
            print("âš ï¸ æ–‡æœ¬å—åˆ—è¡¨ä¸ºç©º")
            return np.array([])
        
        print(f"ğŸ”§ å¼€å§‹å‘é‡åŒ– {len(text_chunks)} ä¸ªæ–‡æœ¬å—...")
        
        all_vectors = []
        processed_count = 0
        
        with paddle.no_grad():
            # åˆ†æ‰¹å¤„ç†
            for i in range(0, len(text_chunks), self.batch_size):
                batch_texts = text_chunks[i:i + self.batch_size]
                batch_vectors = []
                
                for text in batch_texts:
                    if not text or not text.strip():
                        continue
                    
                    try:
                        # å¯¹æ¯ä¸ªæ–‡æœ¬è¿›è¡Œç¼–ç 
                        inputs = self.tokenizer(
                            text,
                            truncation=True,
                            max_length=self.max_length,
                            padding="max_length",
                            return_tensors="pd"
                        )
                        
                        # è·å–æ¨¡å‹è¾“å‡º
                        outputs = self.model(**inputs)
                        
                        # ä½¿ç”¨[CLS] tokençš„å‘é‡ä½œä¸ºæ–‡æœ¬è¡¨ç¤º
                        cls_vector = outputs[0][:, 0, :].numpy()
                        batch_vectors.append(cls_vector[0])
                        
                        processed_count += 1
                        
                    except Exception as e:
                        print(f"âš ï¸ æ–‡æœ¬å‘é‡åŒ–å¤±è´¥ï¼ˆå·²è·³è¿‡ï¼‰: {text[:50]}... - {e}")
                        continue
                
                if batch_vectors:
                    all_vectors.extend(batch_vectors)
                
                # æ˜¾ç¤ºè¿›åº¦
                if i + self.batch_size < len(text_chunks):
                    progress = min(100, int((i + len(batch_texts)) / len(text_chunks) * 100))
                    print(f"  è¿›åº¦: {progress}% ({i + len(batch_texts)}/{len(text_chunks)})")
        
        if all_vectors:
            vectors_array = np.array(all_vectors, dtype=np.float32)
            print(f"âœ… å‘é‡åŒ–å®Œæˆ: {vectors_array.shape[0]} ä¸ªå‘é‡ï¼Œç»´åº¦ {vectors_array.shape[1]}")
            return vectors_array
        else:
            print("âŒ å‘é‡åŒ–å¤±è´¥ï¼šæœªç”Ÿæˆä»»ä½•å‘é‡")
            return np.array([])
    
    def vectorize_single(self, text: str) -> np.ndarray:
        """
        å‘é‡åŒ–å•ä¸ªæ–‡æœ¬
        
        å‚æ•°:
        - text: å•ä¸ªæ–‡æœ¬
        
        è¿”å›:
        - å‘é‡ï¼Œå½¢çŠ¶ä¸º (embedding_dim,)
        """
        if not text or not text.strip():
            raise ValueError("æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        
        vectors = self.vectorize([text])
        if len(vectors) > 0:
            return vectors[0]
        else:
            raise ValueError("å‘é‡åŒ–å¤±è´¥")
    
    def get_embedding_dim(self) -> int:
        """
        è·å–å‘é‡ç»´åº¦
        
        è¿”å›:
        - å‘é‡ç»´åº¦
        """
        return self.embedding_dim
    
    def get_model_info(self) -> Dict[str, str]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        è¿”å›:
        - æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        return {
            "model_name": self.model_name,
            "embedding_dim": self.embedding_dim,
            "max_length": self.max_length,
            "batch_size": self.batch_size
        }
        


# In[42]:


from openai import OpenAI
from typing import List, Dict, Any


# In[44]:


client = OpenAI(
    api_key=os.environ.get("WUXG_API_KEY"),
    base_url="https://aistudio.baidu.com/llm/lmapi/v3"
)


# In[43]:


#wuxg@2025.12.14ï¼šå¦ä¸€ç§ã€åœ¨çº¿å‘é‡åŒ–çš„å®ç°æ–¹å¼
class ERNIEVectorizer2_bak:
    def __init__(self, client):
        self.client = client
        self.model = "embedding-v1"
    
    def get_embedding(self, text: str) -> List[float]:
        response = self.client.embeddings.create(
            model=self.model,
            input=[text]
        )
        print(response)
        print("------------------------")
        print(response.data)
        print("------------------------")
        print(response.data[0])
        print("------------------------")
        print(response.data[0].embedding)
        return response.data[0].embedding
    
    def get_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        response = self.client.embeddings.create(
            model=self.model,
            input=texts
        )
        return [item.embedding for item in response.data]


# In[46]:


#wuxg@2025.12.14ï¼šå¦ä¸€ç§ã€åœ¨çº¿å‘é‡åŒ–çš„å®ç°æ–¹å¼
class ERNIEVectorizer2:
    def __init__(self, client, max_batch_size: int = 16):  # æ–°å¢æ‰¹æ¬¡å¤§å°å‚æ•°
        self.client = client
        self.model = "embedding-v1"
        self.max_batch_size = max_batch_size  # APIå…è®¸çš„æœ€å¤§æ‰¹æ¬¡å¤§å°

    def get_embedding(self, text: str) -> List[float]:
        """è·å–å•ä¸ªæ–‡æœ¬çš„å‘é‡"""
        # ç›´æ¥è°ƒç”¨æ‰¹é‡æ¥å£ï¼Œä½†åªä¼ ä¸€ä¸ªæ–‡æœ¬
        return self.get_embeddings_batch([text])[0]

    def get_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡è·å–æ–‡æœ¬å‘é‡ï¼Œè‡ªåŠ¨å¤„ç†APIçš„æ‰¹æ¬¡é™åˆ¶"""
        all_embeddings = []
        total_texts = len(texts)
        
        print(f"  å‡†å¤‡å‘é‡åŒ– {total_texts} ä¸ªæ–‡æœ¬ï¼Œå°†åˆ†æ‰¹å¤„ç†ï¼ˆæ¯æ‰¹æœ€å¤š {self.max_batch_size} ä¸ªï¼‰...")
        
        # å°†æ–‡æœ¬åˆ—è¡¨æŒ‰max_batch_sizeåˆ†æˆå°æ‰¹æ¬¡
        for i in range(0, total_texts, self.max_batch_size):
            batch = texts[i:i + self.max_batch_size]
            batch_num = i // self.max_batch_size + 1
            total_batches = (total_texts + self.max_batch_size - 1) // self.max_batch_size
            
            print(f"    æ­£åœ¨å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹ ({len(batch)} ä¸ªæ–‡æœ¬)...")
            
            try:
                # è°ƒç”¨API
                response = self.client.embeddings.create(
                    model=self.model,
                    input=batch
                )
                
                # æå–æœ¬æ‰¹æ¬¡çš„å‘é‡
                batch_embeddings = [item.embedding for item in response.data]
                all_embeddings.extend(batch_embeddings)
                
            except Exception as e:
                print(f"    âŒ ç¬¬ {batch_num} æ‰¹å¤„ç†å¤±è´¥: {e}")
                # å¯ä»¥é€‰æ‹©æŠ›å‡ºå¼‚å¸¸ï¼Œæˆ–è€…ç”¨é›¶å‘é‡å¡«å……å¤±è´¥æ‰¹æ¬¡
                # è¿™é‡Œé€‰æ‹©æŠ›å‡ºå¼‚å¸¸ï¼Œç¡®ä¿é—®é¢˜èƒ½è¢«å‘ç°
                raise RuntimeError(f"ç¬¬ {batch_num} æ‰¹å‘é‡åŒ–å¤±è´¥: {e}") from e
        
        print(f"  æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(all_embeddings)} ä¸ªå‘é‡ã€‚")
        return all_embeddings
        


# In[48]:



# ============================================
# 3. åŸºäºFAISSå‘é‡æ•°æ®åº“çš„æ£€ç´¢å™¨ï¼ˆä¿æŒåŸæœ‰ç‰ˆæœ¬ï¼‰
# ============================================
#wuxg@2025.12.14ï¼šæœ¬å‘é‡å¤±è´¥ï¼Œã€å·²å¼ƒç”¨ã€‘å› æ­¤æ”¹ä¸ºåœ¨çº¿å‘é‡åŒ–æ–¹æ¡ˆåã€‚ä½¿ç”¨ä¸‹æ–‡çš„FAISSVectorDB2
class FAISSVectorDB:
    """
    åŸºäºFAISSçš„å‘é‡æ•°æ®åº“ç±»ï¼Œç”¨äºbuildæ„å»ºã€saveå­˜å‚¨ã€loadåŠ è½½ã€å’Œretrieveæ£€ç´¢æ–‡æœ¬å‘é‡(å‚æ•°ï¼šé€‚é… TextProcessorå’ŒERNIEVectorizer2ç±»)
	-  ç‰¹ç‚¹ ï¼š
	  - æ¾è€¦åˆè®¾è®¡ï¼Œä¾èµ–æ³¨å…¥
	  - æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼æ··åˆæ„å»º
	  - å‘é‡ç´¢å¼•çš„æŒä¹…åŒ–å­˜å‚¨
	  - è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯
	  - çµæ´»çš„æ£€ç´¢åŠŸèƒ½

    """
    def __init__(self, embedding_dim: int = 768):
        """
        åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        
        å‚æ•°:
        - embedding_dim: å‘é‡ç»´åº¦ï¼ŒERNIE-3.0-medium-zhä¸º768
        """
        self.embedding_dim = embedding_dim
        self.index = None
        self.chunks = []  # å­˜å‚¨åŸå§‹æ–‡æœ¬å—
        self.metadata = []  # å­˜å‚¨å…ƒæ•°æ®ï¼ˆå¦‚æ–‡æ¡£æ¥æºã€ä½ç½®ç­‰ï¼‰
        self.is_trained = False
        
    def build_from_processor(self, text_processor: TextProcessor, vectorizer: ERNIEVectorizer2,
                           pdf_path: Optional[str] = None, 
                           excel_path: Optional[str] = None, 
                           word_path: Optional[str] = None) -> bool:
        """
        ä½¿ç”¨æ–‡æœ¬å¤„ç†å™¨å’Œå‘é‡åŒ–å™¨æ„å»ºå‘é‡ç´¢å¼•
        
        å‚æ•°:
        - text_processor: æ–‡æœ¬å¤„ç†å™¨å®ä¾‹
        - vectorizer: å‘é‡åŒ–å™¨å®ä¾‹
        - pdf_path: PDFæ–‡ä»¶è·¯å¾„
        - excel_path: Excelæ–‡ä»¶è·¯å¾„
        - word_path: Wordæ–‡ä»¶è·¯å¾„
        
        è¿”å›:
        - æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
        """
        all_chunks = []
        file_sources = []  # è®°å½•æ¯ä¸ªæ–‡æœ¬å—çš„æ¥æº
        
        # ä»PDFæå–
        if pdf_path and os.path.exists(pdf_path):
            print(f"ğŸ“„ ä»PDFæ–‡ä»¶æå–æ–‡æœ¬: {pdf_path}")
            pdf_chunks = text_processor.extract_from_pdf(pdf_path)
            all_chunks.extend(pdf_chunks)
            file_sources.extend(["pdf"] * len(pdf_chunks))
        
        # ä»Excelæå–
        if excel_path and os.path.exists(excel_path):
            print(f"ğŸ“Š ä»Excelæ–‡ä»¶æå–æ–‡æœ¬: {excel_path}")
            excel_chunks = text_processor.extract_from_excel(excel_path)
            all_chunks.extend(excel_chunks)
            file_sources.extend(["excel"] * len(excel_chunks))
        
        # ä»Wordæå–
        if word_path and os.path.exists(word_path):
            print(f"ğŸ“ ä»Wordæ–‡ä»¶æå–æ–‡æœ¬: {word_path}")
            word_chunks = text_processor.extract_from_word(word_path)
            all_chunks.extend(word_chunks)
            file_sources.extend(["word"] * len(word_chunks))
        
        if not all_chunks:
            print("âŒ æœªæå–åˆ°ä»»ä½•æ–‡æœ¬ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„ï¼")
            return False
        
        print(f"âœ… å…±æå–åˆ° {len(all_chunks)} ä¸ªæ–‡æœ¬å—")
        
        # å‘é‡åŒ–æ–‡æœ¬å—
        print("ğŸ”§ æ­£åœ¨è¿›è¡Œæ–‡æœ¬å‘é‡åŒ–...")
        all_vectors = vectorizer.vectorize(all_chunks)
        
        if len(all_vectors) == 0:
            print("âŒ å‘é‡åŒ–å¤±è´¥ï¼Œæ— æœ‰æ•ˆå‘é‡ç”Ÿæˆ")
            return False
        
        # åˆ›å»ºFAISSç´¢å¼•ï¼ˆä½¿ç”¨å†…ç§¯ï¼Œä¾¿äºè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        self.index = faiss.IndexFlatIP(self.embedding_dim)
        
        # å½’ä¸€åŒ–å‘é‡ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ï¼‰
        faiss.normalize_L2(all_vectors)
        
        # æ·»åŠ åˆ°ç´¢å¼•
        self.index.add(all_vectors)
        self.chunks = all_chunks
        
        # åˆ›å»ºå…ƒæ•°æ®
        self.metadata = []
        for i, (chunk, source) in enumerate(zip(all_chunks, file_sources)):
            self.metadata.append({
                "id": i,
                "source": source,
                "chunk_size": len(chunk),
                "preview": chunk[:100] + "..." if len(chunk) > 100 else chunk
            })
        
        print(f"âœ… å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(all_chunks)} ä¸ªå‘é‡")
        print(f"âœ… å‘é‡ç»´åº¦: {self.embedding_dim}")
        print(f"âœ… ç´¢å¼•ç±»å‹: {type(self.index).__name__}")
        
        return True
    
    def retrieve(self, query: str, vectorizer: ERNIEVectorizer2, top_k: int = 5) -> List[Tuple[str, float, Dict]]:
        """
        æ£€ç´¢ä¸æŸ¥è¯¢æœ€ç›¸ä¼¼çš„æ–‡æœ¬å—
        
        å‚æ•°:
        - query: æŸ¥è¯¢æ–‡æœ¬
        - vectorizer: å‘é‡åŒ–å™¨å®ä¾‹
        - top_k: è¿”å›æœ€ç›¸ä¼¼çš„kä¸ªç»“æœ
        
        è¿”å›:
        - åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (æ–‡æœ¬, ç›¸ä¼¼åº¦åˆ†æ•°, å…ƒæ•°æ®)
        """
        if not self.index:
            print("âŒ ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨ build_from_processor æ–¹æ³•")
            return []
        
        if not query or not query.strip():
            print("âŒ æŸ¥è¯¢æ–‡æœ¬ä¸ºç©º")
            return []
        
        # å‘é‡åŒ–æŸ¥è¯¢æ–‡æœ¬
        print(f"ğŸ” å¤„ç†æŸ¥è¯¢: '{query[:50]}...'" if len(query) > 50 else f"ğŸ” å¤„ç†æŸ¥è¯¢: '{query}'")
        query_vector = vectorizer.vectorize([query.strip()])
        
        if len(query_vector) == 0:
            print("âŒ æŸ¥è¯¢å‘é‡åŒ–å¤±è´¥")
            return []
        
        # å½’ä¸€åŒ–æŸ¥è¯¢å‘é‡
        faiss.normalize_L2(query_vector)
        
        # æ£€ç´¢ç›¸ä¼¼æ–‡æœ¬
        similarities, indices = self.index.search(query_vector, top_k)
        
        results = []
        for i, (similarity, idx) in enumerate(zip(similarities[0], indices[0])):
            if idx < len(self.chunks) and idx >= 0:
                chunk = self.chunks[idx]
                metadata = self.metadata[idx] if idx < len(self.metadata) else {}
                results.append((chunk, float(similarity), metadata))
        
        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
        results.sort(key=lambda x: x[1], reverse=True)
        
        print(f"âœ… æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ")
        return results
    
    def save_index(self, filepath: str):
        """
        ä¿å­˜å‘é‡ç´¢å¼•åˆ°æ–‡ä»¶
        
        å‚æ•°:
        - filepath: ä¿å­˜è·¯å¾„ï¼ˆä¸å¸¦æ‰©å±•åï¼‰
        """
        if not self.index:
            print("âŒ ç´¢å¼•æœªæ„å»ºï¼Œæ— æ³•ä¿å­˜")
            return
        
        try:
            # ä¿å­˜FAISSç´¢å¼•
            faiss.write_index(self.index, f"{filepath}.index")
            
            # ä¿å­˜æ–‡æœ¬å’Œå…ƒæ•°æ®
            data_to_save = {
                'chunks': self.chunks,
                'metadata': self.metadata,
                'embedding_dim': self.embedding_dim,
                'is_trained': self.is_trained
            }
            
            with open(f"{filepath}.data", 'wb') as f:
                pickle.dump(data_to_save, f)
            
            print(f"âœ… ç´¢å¼•å·²ä¿å­˜åˆ°: {filepath}.index å’Œ {filepath}.data")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç´¢å¼•å¤±è´¥: {e}")
    
    def load_index(self, filepath: str):
        """
        ä»æ–‡ä»¶åŠ è½½å‘é‡ç´¢å¼•
        
        å‚æ•°:
        - filepath: æ–‡ä»¶è·¯å¾„ï¼ˆä¸å¸¦æ‰©å±•åï¼‰
        """
        try:
            # åŠ è½½FAISSç´¢å¼•
            if os.path.exists(f"{filepath}.index"):
                self.index = faiss.read_index(f"{filepath}.index")
            else:
                print(f"âŒ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {filepath}.index")
                return False
            
            # åŠ è½½æ–‡æœ¬å’Œå…ƒæ•°æ®
            if os.path.exists(f"{filepath}.data"):
                with open(f"{filepath}.data", 'rb') as f:
                    data_loaded = pickle.load(f)
                
                self.chunks = data_loaded['chunks']
                self.metadata = data_loaded['metadata']
                self.embedding_dim = data_loaded['embedding_dim']
                self.is_trained = data_loaded['is_trained']
            else:
                print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {filepath}.data")
                return False
            
            print(f"âœ… ç´¢å¼•åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(self.chunks)} ä¸ªæ–‡æœ¬å—")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
            return False
    
    def get_stats(self) -> Dict:
        """
        è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
        
        è¿”å›:
        - ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not self.index:
            return {"status": "ç´¢å¼•æœªæ„å»º"}
        
        stats = {
            "status": "å·²æ„å»º",
            "total_chunks": len(self.chunks),
            "embedding_dim": self.embedding_dim,
            "index_type": type(self.index).__name__,
            "sources": {}
        }
        
        # ç»Ÿè®¡å„æ¥æºçš„æ–‡æœ¬å—æ•°é‡
        if self.metadata:
            for meta in self.metadata:
                source = meta.get("source", "unknown")
                stats["sources"][source] = stats["sources"].get(source, 0) + 1
        
        return stats


# In[50]:


#wuxg@2025.12.14ï¼šä½¿ç”¨aistudioçš„åœ¨çº¿tokenæ–¹å¼
import numpy as np
import faiss
from typing import List, Dict, Tuple, Optional, Any
import pickle
import os

class FAISSVectorDB2:
    """
    åŸºäºFAISSçš„å‘é‡æ•°æ®åº“ç±»ï¼Œé€‚é…æ–°çš„ ERNIEVectorizer2 APIè°ƒç”¨æ–¹å¼ã€‚
    """
    
    def __init__(self, index_type: str = "flat"):
        """
        åˆå§‹åŒ–å‘é‡æ•°æ®åº“ï¼ˆä¸å†éœ€è¦é¢„è®¾embedding_dimï¼‰
        
        å‚æ•°:
        - index_type: ç´¢å¼•ç±»å‹ï¼Œæ”¯æŒ: "flat" (ç²¾ç¡®æœç´¢), "ivf" (é€‚åˆå¤§è§„æ¨¡)
        """
        self.index_type = index_type
        self.index = None          # FAISSç´¢å¼•å¯¹è±¡
        self.embedding_dim = None  # å‘é‡ç»´åº¦ï¼ˆè¿è¡Œæ—¶ç¡®å®šï¼‰
        self.chunks = []           # å­˜å‚¨åŸå§‹æ–‡æœ¬å—
        self.metadata = []         # å­˜å‚¨å…ƒæ•°æ®
        self.is_trained = False
        self.vectorizer=None
    
    def build_index(self, 
                   text_chunks: List[str], 
                   vectorizer: Any,  # å¯ä¼ å…¥ERNIEVectorizer2å®ä¾‹
                   metadata: Optional[List[Dict]] = None,
                   normalize: bool = True) -> bool:
        """
        æ„å»ºå‘é‡ç´¢å¼•ï¼ˆæ ¸å¿ƒæ–¹æ³•ï¼‰
        
        å‚æ•°:
        - text_chunks: æ–‡æœ¬å—åˆ—è¡¨
        - vectorizer: å‘é‡åŒ–å™¨å®ä¾‹ï¼ˆéœ€æœ‰get_embeddings_batchæ–¹æ³•ï¼‰
        - metadata: å¯é€‰çš„å…ƒæ•°æ®åˆ—è¡¨
        - normalize: æ˜¯å¦å¯¹å‘é‡è¿›è¡ŒL2å½’ä¸€åŒ–ï¼ˆå»ºè®®ä¿æŒTrueï¼Œä»¥ä¾¿ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        
        è¿”å›:
        - æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
        """
        if not text_chunks:
            print("âŒ æ–‡æœ¬å—åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•æ„å»ºç´¢å¼•")
            return False
        
        print(f"ğŸ”§ å¼€å§‹æ„å»ºç´¢å¼•ï¼Œå…± {len(text_chunks)} ä¸ªæ–‡æœ¬å—...")
        
        # 1. æ‰¹é‡å‘é‡åŒ–
        print("  æ­£åœ¨è¿›è¡Œæ–‡æœ¬å‘é‡åŒ–...")
        try:
            # è°ƒç”¨æ–°çš„å‘é‡åŒ–æ¥å£
            embeddings_list = vectorizer.get_embeddings_batch(text_chunks)
        except Exception as e:
            print(f"âŒ å‘é‡åŒ–è¿‡ç¨‹å‡ºé”™: {e}")
            return False
        
        # 2. è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è·å–ç»´åº¦
        try:
            embeddings_array = np.array(embeddings_list, dtype=np.float32)
            self.embedding_dim = embeddings_array.shape[1]
            print(f"âœ… å‘é‡åŒ–å®Œæˆï¼Œç»´åº¦: {self.embedding_dim}")
        except Exception as e:
            print(f"âŒ å‘é‡æ•°æ®è½¬æ¢å¤±è´¥: {e}")
            return False
        
        # 3. åˆ›å»ºFAISSç´¢å¼•
        print(f"  åˆ›å»º {self.index_type} ç±»å‹ç´¢å¼•...")
        try:
            if self.index_type == "flat":
                # ç²¾ç¡®æœç´¢ï¼Œä½¿ç”¨å†…ç§¯åº¦é‡ï¼ˆå½’ä¸€åŒ–åå³ä¸ºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
                self.index = faiss.IndexFlatIP(self.embedding_dim)
            elif self.index_type == "ivf":
                # IVFç´¢å¼•ï¼Œé€‚åˆå¤§è§„æ¨¡æ•°æ®
                nlist = min(100, int(np.sqrt(len(text_chunks))))  # èšç±»ä¸­å¿ƒæ•°
                quantizer = faiss.IndexFlatIP(self.embedding_dim)
                self.index = faiss.IndexIVFFlat(quantizer, self.embedding_dim, nlist, faiss.METRIC_INNER_PRODUCT)
                self.is_trained = False
            else:
                print(f"âŒ ä¸æ”¯æŒçš„ç´¢å¼•ç±»å‹: {self.index_type}")
                return False
        except Exception as e:
            print(f"âŒ åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
            return False
        
        # 4. å½’ä¸€åŒ–å‘é‡ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        if normalize:
            print("  å½’ä¸€åŒ–å‘é‡...")
            faiss.normalize_L2(embeddings_array)
        
        # 5. è®­ç»ƒç´¢å¼•ï¼ˆä»…IVFéœ€è¦ï¼‰
        if self.index_type == "ivf" and len(text_chunks) >= 100:
            print("  è®­ç»ƒIVFç´¢å¼•...")
            try:
                self.index.train(embeddings_array)
                self.is_trained = True
            except Exception as e:
                print(f"âš ï¸  ç´¢å¼•è®­ç»ƒå¤±è´¥: {e}")
                # éƒ¨åˆ†æƒ…å†µä¸‹å¯ç»§ç»­
        
        # 6. æ·»åŠ å‘é‡åˆ°ç´¢å¼•
        print("  æ·»åŠ å‘é‡åˆ°ç´¢å¼•...")
        try:
            self.index.add(embeddings_array)
        except Exception as e:
            print(f"âŒ æ·»åŠ å‘é‡å¤±è´¥: {e}")
            return False
        
        # 7. ä¿å­˜æ–‡æœ¬å’Œå…ƒæ•°æ®
        self.chunks = text_chunks
        self.metadata = metadata if metadata else []
        
        # å¦‚æœå…ƒæ•°æ®ä¸è¶³ï¼Œç”Ÿæˆé»˜è®¤å…ƒæ•°æ®
        if len(self.metadata) < len(self.chunks):
            self.metadata = self.metadata + [
                {"id": i, "chunk_size": len(chunk), "preview": chunk[:100]}
                for i in range(len(self.metadata), len(self.chunks))
            ]
        
        print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼åŒ…å« {self.index.ntotal} ä¸ªå‘é‡")
        return True
    
    def build_from_files(self, 
                        text_processor: Any,
                        vectorizer: Any,
                        pdf_path: Optional[str] = None,
                        excel_path: Optional[str] = None,
                        word_path: Optional[str] = None) -> bool:
        """
        ä»æ–‡ä»¶æ„å»ºç´¢å¼•çš„ä¾¿æ·æ–¹æ³•ï¼ˆå‘åå…¼å®¹ï¼‰
        
        æ³¨æ„ï¼šæ­¤æ–¹æ³•ä¾èµ–TextProcessorï¼Œå¦‚æœä½ çš„é¡¹ç›®ä¸­æ²¡æœ‰ï¼Œå¯åˆ é™¤æˆ–ä¿®æ”¹
        """
        all_chunks = []
        file_sources = []
        self.vectorizer= vectorizer
        
        # ä»ä¸åŒæ–‡ä»¶æå–æ–‡æœ¬
        if pdf_path and os.path.exists(pdf_path):
            print(f"ğŸ“„ ä»PDFæå–: {pdf_path}")
            chunks = text_processor.extract_from_pdf(pdf_path)
            all_chunks.extend(chunks)
            file_sources.extend(["pdf"] * len(chunks))
        
        if excel_path and os.path.exists(excel_path):
            print(f"ğŸ“Š ä»Excelæå–: {excel_path}")
            chunks = text_processor.extract_from_excel(excel_path)
            all_chunks.extend(chunks)
            file_sources.extend(["excel"] * len(chunks))
        
        if word_path and os.path.exists(word_path):
            print(f"ğŸ“ ä»Wordæå–: {word_path}")
            chunks = text_processor.extract_from_word(word_path)
            all_chunks.extend(chunks)
            file_sources.extend(["word"] * len(chunks))
        
        if not all_chunks:
            print("âŒ æœªæå–åˆ°ä»»ä½•æ–‡æœ¬")
            return False
        
        print(f"âœ… å…±æå–åˆ° {len(all_chunks)} ä¸ªæ–‡æœ¬å—")
        
        # åˆ›å»ºå…ƒæ•°æ®
        metadata = [
            {
                "id": i,
                "source": source,
                "chunk_size": len(chunk),
                "preview": chunk[:100] + "..." if len(chunk) > 100 else chunk
            }
            for i, (chunk, source) in enumerate(zip(all_chunks, file_sources))
        ]
        
        # è°ƒç”¨æ ¸å¿ƒæ„å»ºæ–¹æ³•
        return self.build_index(all_chunks, vectorizer, metadata)
    
    def retrieve(self, 
                query: str, 
                vectorizer: Any, 
                top_k: int = 5,
                score_threshold: float = 0.0) -> List[Tuple[str, float, Dict]]:
        """
        æ£€ç´¢ä¸æŸ¥è¯¢æœ€ç›¸ä¼¼çš„æ–‡æœ¬å—
        
        å‚æ•°:
        - query: æŸ¥è¯¢æ–‡æœ¬
        - vectorizer: å‘é‡åŒ–å™¨å®ä¾‹
        - top_k: è¿”å›æœ€ç›¸ä¼¼çš„kä¸ªç»“æœ
        - score_threshold: ç›¸ä¼¼åº¦åˆ†æ•°é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„ç»“æœå°†è¢«è¿‡æ»¤
        
        è¿”å›:
        - åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (æ–‡æœ¬, ç›¸ä¼¼åº¦åˆ†æ•°, å…ƒæ•°æ®)
        """
        if self.index is None:
            print("âŒ ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨ build_index æ–¹æ³•")
            return []
        
        if not query or not query.strip():
            print("âŒ æŸ¥è¯¢æ–‡æœ¬ä¸ºç©º")
            return []
        
        print(f"ğŸ” æŸ¥è¯¢: '{query[:50]}...'" if len(query) > 50 else f"ğŸ” æŸ¥è¯¢: '{query}'")
        
        # 1. å‘é‡åŒ–æŸ¥è¯¢æ–‡æœ¬
        try:
            # ä½¿ç”¨æ–°çš„å‘é‡åŒ–æ¥å£
            query_embedding_list = vectorizer.get_embeddings_batch([query.strip()])
            query_vector = np.array(query_embedding_list[0], dtype=np.float32).reshape(1, -1)
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å‘é‡åŒ–å¤±è´¥: {e}")
            return []
        
        # 2. å½’ä¸€åŒ–æŸ¥è¯¢å‘é‡ï¼ˆå¿…é¡»ä¸ç´¢å¼•æ„å»ºæ—¶çš„å¤„ç†ä¸€è‡´ï¼‰
        faiss.normalize_L2(query_vector)
        
        # 3. æ‰§è¡Œæœç´¢
        try:
            # æ³¨æ„ï¼šIndexFlatIPè¿”å›çš„æ˜¯å†…ç§¯åˆ†æ•°ï¼Œå½’ä¸€åŒ–åå³ä¸ºä½™å¼¦ç›¸ä¼¼åº¦
            scores, indices = self.index.search(query_vector, top_k)
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []
        
        # 4. æ•´ç†ç»“æœ
        results = []
        for score, idx in zip(scores[0], indices[0]):
            # æ£€æŸ¥ç´¢å¼•æœ‰æ•ˆæ€§
            if idx == -1 or idx >= len(self.chunks):
                continue
            
            # åº”ç”¨åˆ†æ•°é˜ˆå€¼
            if score < score_threshold:
                continue
            
            chunk = self.chunks[idx]
            metadata = self.metadata[idx] if idx < len(self.metadata) else {}
            results.append((chunk, float(score), metadata))
        
        print(f"âœ… æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(results)} ä¸ªç»“æœ")
        return results
    
    def save_index(self, filepath: str):
        """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶"""
        if self.index is None:
            print("âŒ ç´¢å¼•æœªæ„å»ºï¼Œæ— æ³•ä¿å­˜")
            return
        
        try:
            # ä¿å­˜FAISSç´¢å¼•
            faiss.write_index(self.index, f"{filepath}.index")
            
            # ä¿å­˜æ•°æ®
            data_to_save = {
                'chunks': self.chunks,
                'metadata': self.metadata,
                'embedding_dim': self.embedding_dim,
                'index_type': self.index_type,
                'is_trained': self.is_trained
            }
            
            with open(f"{filepath}.data", 'wb') as f:
                pickle.dump(data_to_save, f)
            
            print(f"âœ… ç´¢å¼•å·²ä¿å­˜: {filepath}.index, {filepath}.data")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
    
    def load_index(self, filepath: str) -> bool:
        """ä»æ–‡ä»¶åŠ è½½ç´¢å¼•"""
        try:
            # åŠ è½½FAISSç´¢å¼•
            index_path = f"{filepath}.index"
            if not os.path.exists(index_path):
                print(f"âŒ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {index_path}")
                return False
            
            self.index = faiss.read_index(index_path)
            
            # åŠ è½½æ•°æ®
            data_path = f"{filepath}.data"
            if not os.path.exists(data_path):
                print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
                return False
            
            with open(data_path, 'rb') as f:
                data_loaded = pickle.load(f)
            
            self.chunks = data_loaded['chunks']
            self.metadata = data_loaded['metadata']
            self.embedding_dim = data_loaded['embedding_dim']
            self.index_type = data_loaded['index_type']
            self.is_trained = data_loaded['is_trained']
            
            print(f"âœ… ç´¢å¼•åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(self.chunks)} ä¸ªæ–‡æœ¬å—")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            return False
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if self.index is None:
            return {"status": "ç´¢å¼•æœªæ„å»º"}
        
        stats = {
            "status": "å·²æ„å»º",
            "total_chunks": len(self.chunks),
            "embedding_dim": self.embedding_dim,
            "index_type": self.index_type,
            "is_trained": self.is_trained,
            "index_size": self.index.ntotal,
            "sources": {}
        }
        
        # ç»Ÿè®¡æ¥æºåˆ†å¸ƒ
        if self.metadata:
            for meta in self.metadata:
                source = meta.get("source", "unknown")
                stats["sources"][source] = stats["sources"].get(source, 0) + 1
        
        return stats
    
    def similarity_search(self, 
                         query_vector: np.ndarray,
                         top_k: int = 5) -> List[Tuple[int, float]]:
        """
        ç›´æ¥ä½¿ç”¨å‘é‡è¿›è¡Œæœç´¢ï¼ˆé«˜çº§ç”¨æ³•ï¼‰
        
        å‚æ•°:
        - query_vector: å·²å‘é‡åŒ–çš„æŸ¥è¯¢ï¼Œå½¢çŠ¶ä¸º (1, embedding_dim)
        - top_k: è¿”å›æ•°é‡
        
        è¿”å›:
        - åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (ç´¢å¼•, åˆ†æ•°)
        """
        if self.index is None:
            raise ValueError("ç´¢å¼•æœªæ„å»º")
        
        # ç¡®ä¿æŸ¥è¯¢å‘é‡å·²å½’ä¸€åŒ–
        faiss.normalize_L2(query_vector)
        
        scores, indices = self.index.search(query_vector, top_k)
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx != -1:
                results.append((int(idx), float(score)))
        
        return results


# In[48]:



# ============================================
# é˜¶æ®µæ€§å°ç»“ï¼šTextProcess+Embedding+Faiss ä½¿ç”¨ç¤ºä¾‹
# ============================================

def usage_example():  #å·²å¼ƒç”¨ï¼ˆæ”¹ä¸ºåœ¨å‘é‡åŒ–æ–¹å¼ï¼‰
    """ä½¿ç”¨ç¤ºä¾‹"""
    
    # 1. åˆ›å»ºæ–‡æœ¬å¤„ç†å™¨
    print("1. åˆå§‹åŒ–æ–‡æœ¬å¤„ç†å™¨...")
    text_processor = TextProcessor(chunk_size=500)
    
    # 2. åˆ›å»ºERNIEå‘é‡åŒ–å™¨
    print("2. åˆå§‹åŒ–ERNIEå‘é‡åŒ–å™¨...")
    vectorizer = ERNIEVectorizer1(model_name="ernie-3.0-medium-zh", batch_size=8)
    
    # 3. åˆ›å»ºå‘é‡æ•°æ®åº“
    print("3. åˆå§‹åŒ–FAISSå‘é‡æ•°æ®åº“...")
    vector_db = FAISSVectorDB(embedding_dim=vectorizer.get_embedding_dim())
    
    # 4. æ„å»ºç´¢å¼•
    print("4. æ„å»ºå‘é‡ç´¢å¼•...")
    success = vector_db.build_from_processor(
        text_processor=text_processor,
        vectorizer=vectorizer,
        pdf_path="ä¿.pdf",      # æ›¿æ¢ä¸ºå®é™…æ–‡ä»¶è·¯å¾„
        excel_path="å†²ç¨³.xlsx"  # æ›¿æ¢ä¸ºå®é™…æ–‡ä»¶è·¯å¾„
    )
    
    if success:
        # 5. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print("\n5. å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯:")
        stats = vector_db.get_stats()
        for key, value in stats.items():
            if isinstance(value, dict):
                print(f"  {key}:")
                for k, v in value.items():
                    print(f"    {k}: {v}")
            else:
                print(f"  {key}: {value}")
        
        # 6. æµ‹è¯•æ£€ç´¢
        print("\n6. æµ‹è¯•æ£€ç´¢åŠŸèƒ½...")
        test_queries = [
            "ç¾å›½ç•™å­¦çš„ç”³è¯·æˆªæ­¢æ—¥æœŸæ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿ",
            "éœ€è¦å‡†å¤‡å“ªäº›ç”³è¯·ææ–™ï¼Ÿ",
            "ç•™å­¦è´¹ç”¨å¤§æ¦‚æ˜¯å¤šå°‘ï¼Ÿ"
        ]
        
        for query in test_queries:
            print(f"\nğŸ” æŸ¥è¯¢: {query}")
            results = vector_db.retrieve(query, vectorizer, top_k=3)
            
            if results:
                print(f"  æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ:")
                for i, (chunk, score, metadata) in enumerate(results):
                    print(f"  {i+1}. [ç›¸ä¼¼åº¦: {score:.4f}] {chunk[:80]}...")
                    print(f"     æ¥æº: {metadata.get('source', 'unknown')}, å¤§å°: {metadata.get('chunk_size', 0)}å­—ç¬¦")
            else:
                print("  æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
        
        # 7. ä¿å­˜ç´¢å¼•
        print("\n7. ä¿å­˜å‘é‡ç´¢å¼•...")
        vector_db.save_index("faiss_index_example")
        
    else:
        print("âŒ å‘é‡ç´¢å¼•æ„å»ºå¤±è´¥")

if __name__ == "__main__":
    usage_example()


# In[52]:


def usage_example_with_api():
    #ä½¿ç”¨åœ¨çº¿APIçš„ERNIEVectorizer2å’ŒFAISSVectorDBçš„ç¤ºä¾‹"""
    
    print("ğŸš€ å¼€å§‹åŸºäºåœ¨çº¿APIçš„æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)
    
    # 1. åˆ›å»ºæ–‡æœ¬å¤„ç†å™¨
    print("1. åˆå§‹åŒ–æ–‡æœ¬å¤„ç†å™¨...")
    text_processor = TextProcessor(chunk_size=500)
    
    # 2. åˆ›å»ºERNIEVectorizer2 (åœ¨çº¿API)
    print("2. åˆå§‹åŒ–ERNIEVectorizer2 (åœ¨çº¿API)...")
    # æ³¨æ„: ä½ éœ€è¦åœ¨æ­¤å¤„æ­£ç¡®åˆå§‹åŒ–ä½ çš„APIå®¢æˆ·ç«¯
    # å‡è®¾ä½ çš„ERNIEVectorizer2æ¥æ”¶ä¸€ä¸ªå·²é…ç½®çš„clientå¯¹è±¡
    from openai import OpenAI # ç¤ºä¾‹ï¼šä½¿ç”¨OpenAIæ ¼å¼çš„å®¢æˆ·ç«¯
    # è¯·æ›¿æ¢ä¸ºä½ çš„å®é™…APIé…ç½®
    client = OpenAI(
        api_key=os.environ.get("WUXG_API_KEY"),
        base_url="https://aistudio.baidu.com/llm/lmapi/v3"
    )
    vectorizer = ERNIEVectorizer2(client=client)
    print("   âœ… åœ¨çº¿å‘é‡åŒ–å™¨å‡†å¤‡å°±ç»ª")
    
    # 3. åˆ›å»ºå‘é‡æ•°æ®åº“ (é€‚é…ERNIEVectorizer2çš„ç‰ˆæœ¬)
    print("3. åˆå§‹åŒ–FAISSå‘é‡æ•°æ®åº“...")
    # æ³¨æ„: è¿™é‡Œä½¿ç”¨çš„æ˜¯ä½ å·²é‡å†™çš„ã€æ— éœ€é¢„è®¾ç»´åº¦çš„FAISSVectorDBç±»
    vector_db = FAISSVectorDB2(index_type="flat")
    
    # 4. æ„å»ºç´¢å¼•
    print("4. æ„å»ºå‘é‡ç´¢å¼•...")
    success = vector_db.build_from_files(
        text_processor=text_processor,
        vectorizer=vectorizer,
        pdf_path="ä¿.pdf",      # æ›¿æ¢ä¸ºå®é™…æ–‡ä»¶è·¯å¾„
        excel_path="å†²ç¨³.xlsx"  # æ›¿æ¢ä¸ºå®é™…æ–‡ä»¶è·¯å¾„
        # å¯é€‰: word_path="your_doc.docx"
    )
    
    if success:
        # 5. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print("\n5. å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯:")
        stats = vector_db.get_stats()
        for key, value in stats.items():
            if isinstance(value, dict):
                print(f"  {key}:")
                for k, v in value.items():
                    print(f"    {k}: {v}")
            else:
                print(f"  {key}: {value}")
        
        # 6. æµ‹è¯•æ£€ç´¢
        print("\n6. æµ‹è¯•æ£€ç´¢åŠŸèƒ½...")
        test_queries = [
            "ç¾å›½ç•™å­¦çš„ç”³è¯·æˆªæ­¢æ—¥æœŸæ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿ",
            "éœ€è¦å‡†å¤‡å“ªäº›ç”³è¯·ææ–™ï¼Ÿ",
            "ç•™å­¦è´¹ç”¨å¤§æ¦‚æ˜¯å¤šå°‘ï¼Ÿ"
        ]
        
        for query in test_queries:
            print(f"\nğŸ” æŸ¥è¯¢: {query}")
            # æ³¨æ„: retrieveæ–¹æ³•ç°åœ¨æ¥æ”¶ERNIEVectorizer2ä½œä¸ºå‚æ•°
            results = vector_db.retrieve(query, vectorizer, top_k=3)
            
            if results:
                print(f"  æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ:")
                for i, (chunk, score, metadata) in enumerate(results):
                    # æ ¼å¼åŒ–è¾“å‡ºï¼Œæ§åˆ¶é¢„è§ˆé•¿åº¦
                    preview = chunk if len(chunk) <= 80 else chunk[:77] + "..."
                    print(f"  {i+1}. [ç›¸ä¼¼åº¦: {score:.4f}] {preview}")
                    print(f"     æ¥æº: {metadata.get('source', 'unknown')}, å¤§å°: {len(chunk)}å­—ç¬¦")
            else:
                print("  æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
        
        # 7. ä¿å­˜ç´¢å¼• (å¯é€‰)
        print("\n7. ä¿å­˜å‘é‡ç´¢å¼•åˆ°æ–‡ä»¶...")
        vector_db.save_index("faiss_index_online_example")
        print("   âœ… ç´¢å¼•å·²ä¿å­˜ï¼Œå¯ç”¨äºåç»­å¿«é€ŸåŠ è½½")
        
    else:
        print("âŒ å‘é‡ç´¢å¼•æ„å»ºå¤±è´¥")
    
    print("\n" + "=" * 50)
    print("æ¼”ç¤ºç»“æŸ")

if __name__ == "__main__":
    # æ‰§è¡ŒåŸºäºåœ¨çº¿APIçš„ç¤ºä¾‹
    usage_example_with_api()


# In[58]:


# ============================================
#  4.  Self-RAGçŠ¶æ€ä¸å·¥ä½œæµ
# ============================================
class GraphState(TypedDict):
    keys: Dict[str, str]

class SelfRAGGraph:
    def __init__(self, vector_db: FAISSVectorDB2): #ã€å·²å¼ƒç”¨FAISSVectorDBã€‘ä½¿ç”¨FAISSVectorDB2
        self.vector_db = vector_db
        self.nodes = {
            "retrieve": self.retrieve_node,
            "generate": self.generate_node,
            "grade": self.grade_node
        }
        self.entry = "retrieve"

    def retrieve_node(self, state):
        query = state["keys"]["question"]
        print(f"ğŸ” æ£€ç´¢èŠ‚ç‚¹: å¤„ç†æŸ¥è¯¢ '{query}'")
        
        # ä½¿ç”¨å‘é‡æ•°æ®åº“æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved_results = self.vector_db.retrieve(query, self.vector_db.vectorizer,top_k=3)
        
        if retrieved_results:
            # æå–æ£€ç´¢åˆ°çš„æ–‡æœ¬
            documents = [result[0] for result in retrieved_results]
            state["keys"]["documents"] = "\n".join(documents)
            # ä¿å­˜ç›¸ä¼¼åº¦ä¿¡æ¯ä¾›åç»­ä½¿ç”¨
            state["keys"]["retrieval_scores"] = str([result[1] for result in retrieved_results])
            print(f"âœ… æ£€ç´¢åˆ° {len(documents)} ä¸ªç›¸å…³æ–‡æ¡£")
        else:
            state["keys"]["documents"] = "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£"
            state["keys"]["retrieval_scores"] = "[]"
            print("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        
        return state

    def generate_node(self, state):
        print("ğŸ¤– ç”ŸæˆèŠ‚ç‚¹: ç”Ÿæˆç­”æ¡ˆ...")
        documents = state["keys"]["documents"]
        query = state["keys"]["question"]
        
        # ç®€å•ç”Ÿæˆé€»è¾‘ï¼šä»æ–‡æ¡£ä¸­æå–ç›¸å…³ä¿¡æ¯
        if documents != "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£":
            if "æˆªæ­¢æ—¥æœŸ" in query and "æˆªæ­¢æ—¥æœŸ" in documents:
                # æŸ¥æ‰¾åŒ…å«æˆªæ­¢æ—¥æœŸçš„æ–‡æ¡£è¡Œ
                doc_lines = documents.split("\n")
                answer_lines = [line for line in doc_lines if "æˆªæ­¢æ—¥æœŸ" in line]
                answer = answer_lines[0] if answer_lines else "æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°å…·ä½“çš„æˆªæ­¢æ—¥æœŸä¿¡æ¯"
            elif "ç”³è¯·" in query and "ç”³è¯·" in documents:
                # æŸ¥æ‰¾åŒ…å«ç”³è¯·ä¿¡æ¯çš„æ–‡æ¡£è¡Œ
                doc_lines = documents.split("\n")
                answer_lines = [line for line in doc_lines if "ç”³è¯·" in line]
                answer = answer_lines[0] if answer_lines else "æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°å…·ä½“çš„ç”³è¯·ä¿¡æ¯"
            else:
                # é»˜è®¤è¿”å›æ–‡æ¡£æ‘˜è¦
                answer = "æ ¹æ®æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼Œç›¸å…³ä¿¡æ¯å¦‚ä¸‹ï¼š" + documents[:300]
        else:
            answer = "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„æ–‡æ¡£ä¿¡æ¯ã€‚"
        
        state["keys"]["generation"] = answer
        return state

    def grade_node(self, state):
        print("ğŸ“Š è¯„åˆ†èŠ‚ç‚¹: è¯„ä¼°ç”Ÿæˆè´¨é‡...")
        query = state["keys"]["question"]
        generation = state["keys"]["generation"]
        documents = state["keys"]["documents"]
        
        # ç®€å•è¯„åˆ†é€»è¾‘ï¼šæ£€æŸ¥ç”Ÿæˆå†…å®¹æ˜¯å¦åŒ…å«æ–‡æ¡£ä¸­çš„å…³é”®è¯
        # æå–æŸ¥è¯¢å…³é”®è¯ï¼ˆä¸­æ–‡åˆ†è¯ç®€åŒ–ç‰ˆï¼‰
        keywords = []
        for kw in ["æˆªæ­¢æ—¥æœŸ", "ç”³è¯·", "è¦æ±‚", "æ¡ä»¶", "æ—¶é—´", "è´¹ç”¨", "ææ–™"]:
            if kw in query:
                keywords.append(kw)
        
        # å¦‚æœæ²¡æ‰¾åˆ°ç‰¹å®šå…³é”®è¯ï¼Œä½¿ç”¨é€šç”¨è¯
        if not keywords:
            keywords = [word for word in query.split() if len(word) > 1 and word not in ["çš„", "äº†", "åœ¨", "æ˜¯", "æœ‰"]]
        
        # æ£€æŸ¥ç”Ÿæˆå†…å®¹çš„è´¨é‡
        if documents == "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£":
            state["keys"]["final_score"] = "no_documents"
            state["keys"]["assessment"] = "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆå›ç­”"
        elif any(kw in generation for kw in keywords) and len(generation) > 10:
            state["keys"]["final_score"] = "useful"
            state["keys"]["assessment"] = "ç”Ÿæˆå†…å®¹ä¸æŸ¥è¯¢ç›¸å…³ä¸”ä¿¡æ¯å®Œæ•´"
        else:
            state["keys"]["final_score"] = "not_useful"
            state["keys"]["assessment"] = "ç”Ÿæˆå†…å®¹ä¸æŸ¥è¯¢ç›¸å…³æ€§ä¸è¶³"
        
        return state

    def run(self, state):
        print("ğŸš€ å¼€å§‹Self-RAGå·¥ä½œæµ...")
        current = self.entry
        while current:
            print(f"â¡ï¸ å½“å‰èŠ‚ç‚¹: {current}")
            state = self.nodes[current](state)
            # å·¥ä½œæµæµè½¬ï¼šretrieve â†’ generate â†’ grade â†’ end
            if current == "retrieve":
                current = "generate"
            elif current == "generate":
                current = "grade"
            else:
                current = None
        print("ğŸ Self-RAGå·¥ä½œæµå®Œæˆ")
        return state


# In[62]:


# 5. æ‰§è¡Œå…¥å£ï¼ˆæ›´æ–°ä¸ºä½¿ç”¨FAISSVectorDB2ï¼‰
if __name__ == "__main__":
    # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
    print("=" * 50)
    print("ğŸ”§ åˆå§‹åŒ–FAISSå‘é‡æ•°æ®åº“")
    print("=" * 50)

    # 1. åˆ›å»ºæ–‡æœ¬å¤„ç†å™¨
    print("1. åˆå§‹åŒ–æ–‡æœ¬å¤„ç†å™¨...")
    text_processor = TextProcessor(chunk_size=500)
    
    # vector_db = FAISSVectorDB(embedding_dim=768)  # ERNIE-3.0-medium-zhçš„å‘é‡ç»´åº¦#ã€å·²å¼ƒç”¨FAISSVectorDBã€‘ 
    vector_db = FAISSVectorDB2( )  # ERNIE-3.0-medium-zhçš„å‘é‡ç»´åº¦#ã€ã€‘ä½¿ç”¨FAISSVectorDB2 
    
    # æ„å»ºç´¢å¼•ï¼ˆæ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„ï¼‰
    # success = vector_db.build_from_files(#ã€å·²å¼ƒç”¨FAISSVectorDBã€‘ 
    #     pdf_path="ä¿.pdf",      # æ›¿æ¢ä¸ºä½ çš„PDFæ–‡ä»¶è·¯å¾„
    #     excel_path="å†²ç¨³.xlsx"  # æ›¿æ¢ä¸ºä½ çš„Excelæ–‡ä»¶è·¯å¾„
    #     # word_path="example.docx"  # å¦‚æœ‰Wordæ–‡ä»¶å¯æ·»åŠ 
    # )
    
    #ã€ã€‘ä½¿ç”¨FAISSVectorDB2
    vectorizer = ERNIEVectorizer2(client=client)
    vector_db = FAISSVectorDB2(index_type="flat")
    
    # 4. æ„å»ºç´¢å¼•
    print("4. æ„å»ºå‘é‡ç´¢å¼•...")
    success = vector_db.build_from_files(
        text_processor=text_processor,
        vectorizer=vectorizer,
        pdf_path="ä¿.pdf",      # æ›¿æ¢ä¸ºå®é™…æ–‡ä»¶è·¯å¾„
        excel_path="å†²ç¨³.xlsx"  # æ›¿æ¢ä¸ºå®é™…æ–‡ä»¶è·¯å¾„
        # å¯é€‰: word_path="your_doc.docx"
    )

    if not success:
        print("âŒ å‘é‡æ•°æ®åº“æ„å»ºå¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        exit(1)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯:")
    stats = vector_db.get_stats()
    for key, value in stats.items():
        if isinstance(value, dict):
            print(f"  {key}:")
            for k, v in value.items():
                print(f"    {k}: {v}")
        else:
            print(f"  {key}: {value}")
    
    # ä¿å­˜ç´¢å¼•ï¼ˆå¯é€‰ï¼‰
    print("\nğŸ’¾ ä¿å­˜å‘é‡ç´¢å¼•...")
    vector_db.save_index("faiss_index")
    
    # åˆå§‹åŒ–Self-RAGå·¥ä½œæµ
    print("\n" + "=" * 50)
    print("ğŸ§  åˆå§‹åŒ–Self-RAGå·¥ä½œæµ")
    print("=" * 50)
    graph = SelfRAGGraph(vector_db)
    
    # æµ‹è¯•é—®é¢˜
    test_queries = [
        "ç¾å›½ç•™å­¦çš„ç”³è¯·æˆªæ­¢æ—¥æœŸæ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿ",
        "ç”³è¯·éœ€è¦å‡†å¤‡å“ªäº›ææ–™ï¼Ÿ",
        "ç•™å­¦è´¹ç”¨å¤§æ¦‚æ˜¯å¤šå°‘ï¼Ÿ"
    ]
    
    for query in test_queries:
        print(f"\n" + "=" * 50)
        print(f"â“ æµ‹è¯•æŸ¥è¯¢: {query}")
        print("=" * 50)
        
        test_state = {
            "keys": {"question": query}
        }
        
        final_state = graph.run(test_state)
        
        # è¾“å‡ºç»“æœ
        print("\nğŸ“‹ ============Self-RAG ç»“æœ:==================")
        print(f"ã€é—®é¢˜ã€‘ï¼š{final_state['keys']['question']}")
        print(f"ã€æ£€ç´¢çŠ¶æ€ã€‘ï¼š{final_state['keys'].get('retrieval_scores', 'N/A')}")
        print(f"ã€ç”Ÿæˆç­”æ¡ˆã€‘ï¼š{final_state['keys']['generation']}")
        print(f"ã€ç»“æœåˆ¤å®šã€‘ï¼š{final_state['keys']['final_score']} ({final_state['keys'].get('assessment', 'N/A')})")
        print(f"ã€ç›¸å…³æ–‡æ¡£é¢„è§ˆã€‘ï¼š{final_state['keys']['documents'][:200]}..." if len(final_state['keys']['documents']) > 200 else f"ç›¸å…³æ–‡æ¡£ï¼š{final_state['keys']['documents']}")


# è¯·ç‚¹å‡»[æ­¤å¤„](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)æŸ¥çœ‹æœ¬ç¯å¢ƒåŸºæœ¬ç”¨æ³•.  <br>
# Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. 
	